# Market Data Pipeline (Stocks, Crypto & Forex)

## ğŸ“Œ Project Description

This project demonstrates how to build a **simple, end-to-end market data pipeline** using Python.
The focus is on **data engineering fundamentals** rather than complex modeling or automation.

The pipeline covers the full lifecycle of market data:
from **data ingestion** to **cleaning, transformation, analysis, visualization, and reporting**.

This project is designed to show how raw financial data can be transformed into a clean,
structured, and analyzable format using clear and beginner-friendly logic.

---

## ğŸ¯ Project Objective

The main objective of this project is to:

* Build a **foundational data pipeline**
* Apply consistent data structures across multiple asset classes
* Perform basic feature engineering and anomaly detection
* Produce clean analytical outputs suitable for further modeling or strategy development

---

## ğŸ“Š Data Sources & Assets

### Market Data Source

* Yahoo Finance (via `yfinance` Python library)

### Asset Classes Covered

**Equities & Indices**

* NIFTY 50
* Bank Nifty
* Selected Indian stocks

**Cryptocurrency**

* Bitcoin (BTC)
* Ethereum (ETH)

**Forex**

* EUR/USD
* USD/JPY

---

## ğŸ§± Pipeline Design

The project follows a **batch-based data pipeline architecture**:

### 1. Data Ingestion

* Historical market data downloaded programmatically
* Two-year rolling time window
* Unified download logic across assets

### 2. Data Structuring

All datasets are normalized into a single schema:

* Date
* Open
* High
* Low
* Close
* Volume
* Symbol
* Asset_Type

This ensures consistency and simplifies downstream analysis.

---

### 3. Data Cleaning & Validation

* Fixed index and header issues
* Converted date columns to proper datetime format
* Removed missing and inconsistent records
* Ensured consistent data types across datasets

---

### 4. Feature Engineering

Basic analytical features were added, including:

* Daily returns
* Moving averages
* RSI
* Volume-based metrics

These features serve as inputs for exploratory analysis and anomaly detection.

---

### 5. Anomaly Detection (Rule-Based)

Simple rule-based logic was used to detect unusual market behavior:

* Volume anomalies using a 3Ã— average volume threshold
* Price spikes using daily return thresholds
* Combined anomalies (price + volume)

This approach demonstrates how anomalies can be flagged without machine learning.

---

### 6. Visualization

Clear and interpretable charts were generated:

* Price trends
* Technical indicators
* Volume anomaly highlighting

These visualizations help validate data quality and analytical results.

---

### 7. Outputs & Reporting

* Cleaned datasets ready for analysis
* Separate anomaly tables
* Visual outputs suitable for reporting or presentations

---

## ğŸ› ï¸ Tools & Technologies

* Python
* Pandas
* NumPy
* Matplotlib
* yFinance
* Jupyter Notebook

---
## ğŸ“ Project Structure

```
market-data-pipeline/
â”‚
â”œâ”€â”€ Day_2_Stock_Data/
â”‚   â”œâ”€â”€ Day_2_Stock_Data_Collection.ipynb
â”‚   â”œâ”€â”€ BankNifty_1000_Sessions.csv
â”‚   â””â”€â”€ Sensex_1000_Sessions.csv
â”‚
â”œâ”€â”€ Day_3_Crypto_Forex_Data/
â”‚   â””â”€â”€ Day_3_Crypto_Forex_Data_Collection.ipynb
â”‚
â”œâ”€â”€ Day_4_Cleaned_Data/
â”‚   â””â”€â”€ Day_4_Data_Cleaning_&_Validation.ipynb
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ Day_5_Basic_Technical_Analysis.ipynb
â”‚   â”œâ”€â”€ Day_6_Anomaly_Detection_Prep.ipynb
â”‚   â”œâ”€â”€ day1_api_connectivity.ipynb
â”‚   â””â”€â”€ banknifty_sensex_opening_data.ipynb
â”‚
â”œâ”€â”€ reports/
â”‚   â”œâ”€â”€ Day_2_StockData_Verification_Report.pdf
â”‚   â””â”€â”€ Day_2_StockData_Verification_Report.pbix
â”‚
â”œâ”€â”€ data_exports/
â”‚   â””â”€â”€ Day_2_Stock_Data.rar
â”‚
â”œâ”€â”€ .gitignore
â”œâ”€â”€ app.log
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

### ğŸ“Œ Folder Description

* **Day_2_Stock_Data/**
  Stock market data collection notebooks and exported CSV files.

* **Day_3_Crypto_Forex_Data/**
  Cryptocurrency and forex historical data collection.

* **Day_4_Cleaned_Data/**
  Cleaned and validated datasets with consistent structure.

* **notebooks/**
  Technical analysis, anomaly detection, and utility notebooks.

* **reports/**
  PDF and Power BI reports used for validation and documentation.

* **data_exports/**
  Archived data outputs for sharing or backup.

* **requirements.txt**
  Python dependencies required to run the project.

* **app.log**
  Execution logs generated during data processing.

---

## ğŸ§  Notes

* The project follows a **step-by-step data pipeline approach**
* Each folder represents a logical stage in the pipeline
* The structure prioritizes clarity and learning over automation

---

## ğŸ¯ Purpose of This Structure

This layout is designed to clearly demonstrate how a simple
market data pipeline can be built incrementally, from raw data
collection to cleaned datasets, analysis, and reporting.


---

## âš ï¸ Limitations

* Data sourced from public APIs (not exchange-grade)
* Pipeline is batch-based, not real-time
* Anomaly detection is threshold-based
* Designed for learning and foundational demonstration

---

## ğŸš€ Future Improvements

* Automate execution using scheduling tools
* Store data in a database or data lake
* Add statistical or ML-based anomaly detection
* Integrate real-time market feeds
* Extend pipeline monitoring and logging

---

## ğŸ‘¤ Author

**Mohamed Hegazy**
Data Analyst
